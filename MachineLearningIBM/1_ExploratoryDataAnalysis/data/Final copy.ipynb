{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Part a: Reading Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Objective(s)\n",
    "\n",
    " - Create a SQL database connection to a sample SQL database, and read records from that database\n",
    " - Explore common input parameters\n",
    "\n",
    "### Packages\n",
    "\n",
    " - [Pandas](https://pandas.pydata.org/pandas-docs/stable/)\n",
    " - [Pandas.read_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html)\n",
    " - [SQLite3](https://docs.python.org/3.6/library/sqlite3.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple data reads\n",
    "\n",
    "Structured Query Language (SQL) is an [ANSI specification](https://docs.oracle.com/database/121/SQLRF/ap_standard_sql001.htm#SQLRF55514), implemented by various databases. SQL is a powerful format for interacting with large databases efficiently, and SQL allows for a consistent experience across a large market of databases. We'll be using sqlite, a lightweight and somewhat restricted version of sql for this example. sqlite uses a slightly modified version of SQL, which may be different than what you're used to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3 as sq3\n",
    "# import pandas.io.sql as pds\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Database connections\n",
    "\n",
    "Our first step will be to create a connection to our SQL database. A few common SQL databases used with Python include:\n",
    "\n",
    " - Microsoft SQL Server\n",
    " - Postgres\n",
    " - MySQL\n",
    " - AWS Redshift\n",
    " - AWS Aurora\n",
    " - Oracle DB\n",
    " - Terradata\n",
    " - Db2 Family\n",
    " - Many, many others\n",
    " \n",
    "Each of these databases will require a slightly different setup, and may require credentials (username & password), tokens, or other access requirements. We'll be using `sqlite3` to connect to our database, but other connection packages include:\n",
    "\n",
    " - [`SQLAlchemy`](https://www.sqlalchemy.org/) (most common)\n",
    " - [`psycopg2`](http://initd.org/psycopg/)\n",
    " - [`MySQLdb`](http://mysql-python.sourceforge.net/MySQLdb.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize path to SQLite database\n",
    "path = 'data/classic_rock.db'\n",
    "con = sq3.Connection(path)\n",
    "\n",
    "# We now have a live connection to our SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data\n",
    "\n",
    "Now that we've got a connection to our database, we can perform queries, and load their results in as Pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the query\n",
    "query = '''\n",
    "SELECT * \n",
    "FROM rock_songs;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations = pd.read_sql(query, con)\n",
    "\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also run any supported SQL query\n",
    "# Write the query\n",
    "query = '''\n",
    "SELECT Artist, Release_Year, COUNT(*) AS num_songs, AVG(PlayCount) AS avg_plays  \n",
    "    FROM rock_songs\n",
    "    GROUP BY Artist, Release_Year\n",
    "    ORDER BY num_songs desc;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations = pd.read_sql(query, con)\n",
    "\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common parameters\n",
    "\n",
    "There are a number of common paramters that can be used to read in SQL data with formatting:\n",
    "\n",
    " - coerce_float: Attempt to force numbers into floats\n",
    " - parse_dates: List of columns to parse as dates\n",
    " - chunksize: Number of rows to include in each chunk\n",
    " \n",
    "Let's have a look at using some of these parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query='''\n",
    "SELECT Artist, Release_Year, COUNT(*) AS num_songs, AVG(PlayCount) AS avg_plays  \n",
    "    FROM rock_songs\n",
    "    GROUP BY Artist, Release_Year\n",
    "    ORDER BY num_songs desc;\n",
    "'''\n",
    "\n",
    "# Execute the query\n",
    "observations_generator = pd.read_sql(query,\n",
    "                            con,\n",
    "                            coerce_float=True, # Doesn't effect this dataset, because floats were correctly parsed\n",
    "                            parse_dates=['Release_Year'], # Parse `Release_Year` as a date\n",
    "                            chunksize=5 # Allows for streaming results as a series of shorter tables\n",
    "                            )\n",
    "\n",
    "print(observations_generator)\n",
    "\n",
    "for index, observations in enumerate(observations_generator):\n",
    "    if index < 5:\n",
    "        print(f'Observations index: {index}')\n",
    "        display(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Part b: Reading Data Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sqlite3 as sq3\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Exercise: Reading in database files\n",
    "\n",
    " - Create a variable, `path`, containing the path to the `baseball.db` contained in `resources/`\n",
    " - Create a connection, `con`, that is connected to database at `path`\n",
    " - Create a variable, `query`, containing a SQL query which reads in all data from the `allstarfull` table\n",
    " - Create a variable, `observations`, by using pandas' [read_sql](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_sql.html)\n",
    "\n",
    "### Optional\n",
    " - Create a variable, `tables`, which reads in all data from the table `sqlite_master`\n",
    " - Pretend that you were interesting in creating a new baseball hall of fame. Join and analyze the tables to evaluate the top 3 all time best baseball players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Create a variable, `path`, containing the path to the `baseball.db` contained in `resources/`\n",
    "path = 'data/baseball.db'\n",
    "\n",
    "# Create a connection, `con`, that is connected to database at `path`\n",
    "con = sq3.Connection(path)\n",
    "\n",
    "# Create a variable, `query`, containing a SQL query which reads in all data from the `` table\n",
    "\n",
    "query = \"\"\"\n",
    "SELECT *\n",
    "    FROM allstarfull\n",
    "    ;\n",
    "\"\"\"\n",
    "\n",
    "observations = pd.read_sql(query, con)\n",
    "\n",
    "print(observations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a variable, tables, which reads in all data from the table sqlite_master\n",
    "all_tables = pd.read_sql('SELECT * FROM sqlite_master', con)\n",
    "print(all_tables)\n",
    "print()\n",
    "\n",
    "# Pretend that you were interesting in creating a new baseball hall of fame. Join and analyze the tables to evaluate the top 3 all time best baseball players\n",
    "best_query = \"\"\"\n",
    "SELECT playerID, sum(GP) AS num_games_played, AVG(startingPos) AS avg_starting_position\n",
    "    FROM allstarfull\n",
    "    GROUP BY playerID\n",
    "    ORDER BY num_games_played DESC, avg_starting_position ASC\n",
    "    LIMIT 3\n",
    "\"\"\"\n",
    "best = pd.read_sql(best_query, con)\n",
    "print(best.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n",
    "Estimated time needed: **45** minutes\n",
    "\n",
    "Most of the real-world data, that the data scientist work with, are raw data, meaning that it can contain repeated, missing, and irrelevant entries of information. Hence, if this data is used in any machine learning analysis, it will result in low accuracy or incorrect prediction. For this reason, data cleaning, also known as data cleansing, is an important technique that comes prior to any model building. \n",
    "\n",
    "In this notebook, we will take a look at some of the common data cleaning techniques that data scientists may use to prepare their data for analysis.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "After completing this lab you will be able to:\n",
    "\n",
    "*   Use Log function to transform the data\n",
    "*   Handle the duplicates\n",
    "*   Handle the missing values\n",
    "*   Standardize and normalize the data\n",
    "*   Handle the outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    " - [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for managing the data.\n",
    " - [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for mathematical operations.\n",
    " - [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for visualizing the data.\n",
    " - [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for visualizing the data.\n",
    " - [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for machine learning and machine-learning-pipeline related functions.\n",
    " - [`scipy`](https://docs.scipy.org/doc/scipy/tutorial/stats.html/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) for statistical computations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import the required libraries**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import seaborn as sns \n",
    "import matplotlib.pylab as plt\n",
    "# Any plot generated will be displayed in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the display width so that all columns are displayed\n",
    "# pd.set_option('display.width', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading and understanding our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the Ames_Housing_Data.tsv file, hosted on IBM Cloud object storage. The Ames housing dataset examines features of houses sold in Ames (a small city in the state of Iowa in the United States) during the 2006â€“2010 timeframe.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into *pandas* data frame and look at the first 5 rows using the `head()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = pd.read_csv(\"data/Ames_Housing_Data.tsv\", sep='\\t')\n",
    "housing.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find more information about the features and types using the `info()`  method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the output above, we have 2931 entries, 0 to 2930, as well as 81 features. The \"Non-Null Count\" column shows the number of non-null entries.  If the count is 2931 then there is no missing values for that particular feature. 'SalePrice' is our target or response variable and the rest of the features are our predictor variables.\n",
    "\n",
    "We also have a mix of numerical (28 int64 and 11 float64) and object data types. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the `describe()` function to show the count, mean, min, max of the sale price attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"SalePrice\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis, it is important to note that the minimum value is greater than 0. Also, there is a big difference between the minimum value and the 25th percentile. It is bigger than the 75th percentile and the maximum value. This means that our data might not be normally distributed (an important assumption for linear regression analysis), so will check for normality in the Log Transform section. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function reveals the statistical information about the numeric attributes. To reveal some information about our categorical (object) attributes, we can use `value_counts()` function. In this exercise, describe all categories of the 'Sale Condition' attribute.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "housing[\"Sale Condition\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    " housing[\"Sale Condition\"].value_counts()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Looking for Correlations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before proceeding with the data cleaning, it is useful to establish a correlation between the response variable (in our case the sale price) and other predictor variables, as some of them might not have any major impact in determining the price of the house and will not be used in the analysis.  There are many ways to discover correlation between the target variable and the rest of the features. Building pair plots, scatter plots, heat maps, and a correlation matrixes are the most common ones. Below, we will use the `corr()` function to list the top features based on the [pearson correlation coefficient](https://en.wikipedia.org/wiki/Pearson_correlation_coefficient?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) (measures how closely two sequences of numbers are correlated). Correlation coefficient can only be calculated on the numerical attributes (floats and integers), therefore, only the numeric attributes will be selected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the columns having numerical data types\n",
    "house_num = housing.select_dtypes(include = ['float64', 'int64'])\n",
    "\n",
    "# .corr()               -- Finds the pairwise correlation of all columns\n",
    "# ['SalePrice']         -- Select only the correlation of SalePrice with other variables\n",
    "# [:-1]                 -- Slice the last row because it contains the correlation of SalePrice with SalePrice which is always 1\n",
    "house_num_corr = house_num.corr()['SalePrice'][:-1]\n",
    "\n",
    "# abs()                 -- Find the absolute value of all the correlations because we are interested in the magnitude of the correlation\n",
    "# > 0.5                 -- Select only the correlations whose magnitude is greater than 0.5 because they are considered to be strongly correlated\n",
    "# .sort_values          -- Sort the values in descending order\n",
    "top_features = house_num_corr[abs(house_num_corr) > 0.5].sort_values(ascending=False)\n",
    "print(f'There is {len(top_features)} strongly correlated values with SalePrice:\\n{top_features}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, there are 11 features, with coefficients greater than 0.5, that are strongly correlated with the sale price. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's generate some pair plots to visually inspect the correlation between some of these features and the target variable. We will use seaborns `sns.pairplot()` function for this analysis. Also, building pair plots is one of the possible ways to spot the outliers that might be present in the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(house_num.columns),5):\n",
    "\n",
    "    sns.pairplot(data=house_num,\n",
    "                x_vars=house_num.columns[i:i+5],\n",
    "                y_vars=['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Pearsons Correlation Coefficients and pair plots, we can draw some conclusions about the features that are most strongly correlated to the 'SalePrice'. They are: 'Overall Qual', 'Gr Liv Area', 'Garage Cars', 'Garage Area', and others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Log Transformation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we are going to inspect whether our 'SalePrice' data is normally distributed. The assumption of the normal distribution must be met in order to perform any type of regression analysis. There are several ways to check for this assumption, however here, we will use the visual method, by plotting the 'SalePrice' distribution using the `distplot()` function from the `seaborn` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_untransformed = sns.displot(housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the plot shows, our 'SalePrice' deviates from the normal distribution. It has a longer tail to the right, so we call it a positive skew. In statistics *skewness* is a measure of asymmetry of the distribution. In addition to skewness, there is also a kurtosis, parameter which refers to the pointedness of a peak in the distribution curve. Both skewness and kurtosis are frequently used together to characterize the distribution of data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we can simply use the `skew()` function to calculate our skewness level of the `SalePrice`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % housing['SalePrice'].skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The range of skewness for a fairly symmetrical bell curve distribution is between -0.5 and 0.5; moderate skewness is -0.5 to -1.0 and 0.5 to 1.0; and highly skewed distribution is < -1.0 and > 1.0. In our case, we have ~1.7, so it is considered  highly skewed data. \n",
    "\n",
    "Now, we can try to transform our data, so it looks more normally distributed. We can use the `np.log()` function from the `numpy` library to perform log transform. This [documentation](https://numpy.org/doc/stable/reference/generated/numpy.log.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) contains more information about the numpy log transform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_transformed = np.log(housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_transformed = sns.displot(log_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % (log_transformed).skew())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the log method transformed the 'SalePrice' distribution into a more symmetrical bell curve and the skewness level now is -0.01, well within the range. \n",
    "\n",
    "There are other ways to correct for skewness of the data. For example, Square Root Transform (`np.sqrt`) and the Box-Cox Transform (`stats.boxcox` from the `scipy stats` library). To learn more about these two methods, please check out this [article](https://towardsdatascience.com/top-3-methods-for-handling-skewed-data-1334e0debf45?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, visually inspect the 'Lot Area' feature. If there is any skewness present, apply log transform to make it more normally distributed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "print(f'Skewness of Lot Area: {housing['Lot Area'].skew()}. First plot shows the distribution of Lot Area')\n",
    "sns.displot(housing['Lot Area'])\n",
    "\n",
    "transformed_lot_area = np.log(housing['Lot Area'])\n",
    "print(f'Skewness of transformed Lot Area: {transformed_lot_area.skew()}. Second plot shows the distribution of transformed Lot Area')\n",
    "sns.displot(transformed_lot_area)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "la_plot = sns.distplot(housing['Lot Area'])\n",
    "print(\"Skewness: %f\" % housing['Lot Area'].skew())\n",
    "la_log = np.log(housing['Lot Area'])\n",
    "print(\"Skewness: %f\" % la_log.skew())\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Duplicates**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the video, having duplicate values can effect our analysis, so it is good to check whether there are any duplicates in our data. We will use pandas `duplicated()` function and search by the 'PID' column, which contains a unique index number for each entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicate = housing[housing.duplicated(['PID'])]\n",
    "duplicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there is one duplicate row in this dataset. To remove it, we can use pandas `drop_duplicates()` function. By default, it removes all duplicate rows based on all the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_removed = housing.drop_duplicates()\n",
    "dup_removed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative way to check if there are any duplicated Indexes in our dataset is using `index.is_unique` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise try to remove duplicates on a specific column by setting the subset equal to the column that contains the duplicate, such as 'Order'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "#Check if the 'Order' column has duplicate values\n",
    "print(housing['Order'].is_unique)\n",
    "\n",
    "# Yes, the 'Order' column has duplicate values\n",
    "# Find the duplicate values in the 'Order' column\n",
    "print(housing[housing.duplicated(['Order'])])\n",
    "\n",
    "# Drop the duplicate values in the 'Order' column\n",
    "housing.drop_duplicates(subset=['Order'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "removed_sub = housing.drop_duplicates(subset=['Order'])\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Missing Values**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Missing Values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier detection of missing values, pandas provides the `isna()`, `isnull()`, and `notna()` functions. For more information on pandas missing values please check out this [documentation](https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize all the missing values in our dataset, we will use `isnull()` function. Then, we will add them all up, by using `sum()` function, sort them with `sort_values()` function, and plot the first 20 columns (as the majority of our missing values fall within first 20 columns), using the `bar plot` function from the `matplotlib` library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = housing.isnull().sum().sort_values(ascending=False)\n",
    "total_select = total.head(20)\n",
    "total_select.plot(kind=\"bar\")\n",
    "\n",
    "plt.xlabel(\"Columns\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Total Missing Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several options for dealing with missing values. We will use 'Lot Frontage' feature to analyze for missing values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We can drop the missing values, using `dropna()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.dropna(subset=[\"Lot Frontage\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, all the rows, containing null values in 'Lot Frontage' feature, for example, will be dropped. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We can drop the whole attribute (column), that contains missing values, using the `drop()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.drop(\"Lot Frontage\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this method, the entire column containing the null values will be dropped. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We can replace the missing values (zero, the mean, the median, etc.), using `fillna()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median = housing[\"Lot Frontage\"].median()\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"Lot Frontage\"] = housing[\"Lot Frontage\"].fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Index# 2927, containing a missing value in the \"Lot Frontage\", now has been replaced with the median value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, let's look at 'Mas Vnr Area' feature and replace the missing values with the mean value of that column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "housing['Mas Vnr Area'] = housing['Mas Vnr Area'].fillna(housing['Mas Vnr Area'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "mean = housing[\"Mas Vnr Area\"].mean()\n",
    "housing[\"Mas Vnr Area\"].fillna(mean, inplace = True)   \n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Scaling**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most important transformations we need to apply to our data is feature scaling.  There are two common ways to get all attributes to have the same scale: min-max scaling and standardization.\n",
    "\n",
    "Min-max scaling (or normalization) is the simplest: values are shifted and rescaled so they end up ranging from 0 to 1. This is done by subtracting the min value and dividing by the max minus min.\n",
    "\n",
    "Standardization is different: first it subtracts the mean value (so standardized values always have a zero mean), and then it divides by the standard deviation, so that the resulting distribution has unit variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn library provides `MinMaxScaler` for normalization and `StandardScaler` for standardization needs. For more information on `scikit-learn` [`MinMaxScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) and [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) please visit their respective documentation websites. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will normalize our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_data = MinMaxScaler().fit_transform(house_num)\n",
    "norm_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the data is now a `ndarray` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can also standardize our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = StandardScaler().fit_transform(house_num)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, use `StandardScaler()` and `fit_transform()` functions to standardize the 'SalePrice' feature only.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "housing['SalePrice'] = StandardScaler().fit_transform(housing[['SalePrice']])\n",
    "housing['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "scaled_sprice = StandardScaler().fit_transform(housing['SalePrice'][:,np.newaxis]) \n",
    "scaled_sprice\n",
    "</code>\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Handling the Outliers**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In statistics, an outlier is an observation point that is distant from other observations. An outlier can be due to some mistakes in data collection or recording, or due to natural high variability of data points. How to treat an outlier highly depends on our data or the type of analysis to be performed. Outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n",
    "\n",
    "There are many ways to discover outliers in our data. We can do Uni-variate analysis (using one variable analysis) or Multi-variate analysis (using two or more variables). One of the simplest ways to detect an outlier is to inspect the data visually, by making box plots or scatter plots. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uni-variate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box plot is a method for graphically depicting groups of numerical data through their quartiles. Box plots may also have lines extending vertically from the boxes (whiskers) indicating variability outside the upper and lower quartiles. Outliers may be plotted as individual points. To learn more about box plots please click [here](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.boxplot.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will use a box plot for the 'Lot Area' and the 'SalePrice' features. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['Lot Area'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x=housing['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from these two plots, we have some points that are plotted outside the box plot area and that greatly deviate from the rest of the population. Whether to remove or keep them will greatly depend on the understanding of our data and the type of analysis to be performed. In this case, the points that are outside of our box plots in the 'Lot Area' and the 'Sale Price' might be the actual true data points and do not need to be removed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-variate Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at the bi-variate analysis of the two features, the sale price, 'SalePrice', and the ground living area, 'GrLivArea', and plot the scatter plot of the relationship between these two parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_area = housing.plot.scatter(x='Gr Liv Area',\n",
    "                      y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above graph, there are two values above 5000 sq. ft. living area that deviate from the rest of the population and do not seem to follow the trend. It can be speculated why this is happening but for the purpose of this lab we can delete them. \n",
    "\n",
    "The other two observations on the top are also deviating from the rest of the points but they also seem to be following the trend, so, perhaps, they can be kept.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting the Outliers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will sort all of our 'Gr Liv Area' values and select only the last two.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.sort_values(by = 'Gr Liv Area', ascending = False)[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will use the pandas `drop()` function to remove these two rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_dropped = housing.drop(housing.index[[1499,2181]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_plot = outliers_dropped.plot.scatter(x='Gr Liv Area',\n",
    "                                         y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we do not have the last two points of the 'Gr Liv Area' anymore.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, determine whether there are any outliers in the 'Lot Area' feature. You can either plot the box plot for the 'Lot Area', perform a bi-variate analysis by making a scatter plot between the 'SalePrice' and the 'Lot Area', or use the Z-score analysis. If there are any outliers, remove them from the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "sns.boxplot(x=housing['Lot Area'])\n",
    "\n",
    "new_plot = housing.plot.scatter(x='Lot Area',\n",
    "                                        y='SalePrice')\n",
    "\n",
    "outliers_dropped = housing.drop(housing[housing['Lot Area'] > 100000].index)\n",
    "\n",
    "new_plot = outliers_dropped.plot.scatter(x='Lot Area',\n",
    "                                        y='SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "sns.boxplot(x=housing['Lot Area'])\n",
    "price_lot = housing.plot.scatter(x='Lot Area', y='SalePrice')   \n",
    "housing['Lot_Area_Stats'] = stats.zscore(housing['Lot Area'])\n",
    "housing[['Lot Area','Lot_Area_Stats']].describe().round(3)\n",
    "housing.sort_values(by = 'Lot Area', ascending = False)[:1]\n",
    "lot_area_rem = housing.drop(housing.index[[957]])\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Answer</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "There seems to be one outlier, the very last point in the 'Lot Area' is too far from the rest of the group. Also, according to the Z-score, the standard deviation of that point exceeds the threshhold of 3.\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Z-score Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z-score is another way to identify outliers mathematically. Z-score is the signed number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. In another words, Z-score is the value that quantifies relationship between a data point and a standard deviation and mean values of a group of points. Data points which are too far from zero will be treated as the outliers. In most of the cases, a threshold of 3 or -3 is used. For example, if the Z-score value is greater than or less than 3 or -3 standard deviations respectively, that data point will be identified as a outlier.\n",
    "\n",
    "To learn more about Z-score, please visit this [Wikipedia](https://en.wikipedia.org/wiki/Standard_score?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01) site. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are using Z-score function from `scipy` library to detect the outliers in our 'Low Qual Fin SF' parameter. To learn more about `scipy.stats`, please visit this [link](https://docs.scipy.org/doc/scipy/reference/tutorial/stats.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork837-2023-01-01).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing['LQFSF_Stats'] = stats.zscore(housing['Low Qual Fin SF'])\n",
    "housing[['Low Qual Fin SF','LQFSF_Stats']].describe().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scaled results show a mean of 0.000 and a standard deviation of 1.000, indicating that the transformed values fit the z-scale model. The max value of 22.882 is further proof of the presence of outliers, as it falls well above the z-score limit of +3.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Part c: EDA Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "We will be using the iris data set for this tutorial. This is a well-known data set containing iris species and sepal and petal measurements. The data we will use are in a file called `iris_data.csv` found in the [data](data/) directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "\n",
    "Load the data from the file using the techniques learned today. Examine it.\n",
    "\n",
    "Determine the following:\n",
    "\n",
    "* The number of data points (rows). (*Hint:* check out the dataframe `.shape` attribute.)\n",
    "* The column names. (*Hint:* check out the dataframe `.columns` attribute.)\n",
    "* The data types for each column. (*Hint:* check out the dataframe `.dtypes` attribute.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"data/iris_data.csv\"\n",
    "data = pd.read_csv(filepath)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Number of rows\n",
    "print(f'(rows, column) = {data.shape}')\n",
    "\n",
    "# Column names\n",
    "print(data.columns.tolist())\n",
    "\n",
    "# Data types\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "Examine the species names and note that they all begin with 'Iris-'. Remove this portion of the name so the species name is shorter. \n",
    "\n",
    "*Hint:* there are multiple ways to do this, but you could use either the [string processing methods](http://pandas.pydata.org/pandas-docs/stable/text.html) or the [apply method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.apply.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# The str method maps the following function to each entry as a string\n",
    "data['species'] = data.species.str.replace('Iris-', '')\n",
    "# alternatively\n",
    "# data['species'] = data.species.apply(lambda r: r.replace('Iris-', ''))\n",
    "\n",
    "data.head()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "\n",
    "Determine the following:  \n",
    "* The number of each species present. (*Hint:* check out the series `.value_counts` method.)\n",
    "* The mean, median, and quantiles and ranges (max-min) for each petal and sepal measurement.\n",
    "\n",
    "*Hint:* for the last question, the `.describe` method does have median, but it's not called median. It's the *50%* quantile. `.describe` does not have range though, and in order to get the range, you will need to create a new entry in the `.describe` table, which is `max - min`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# One way to count each species\n",
    "data['species'].value_counts()\n",
    "#OR\n",
    "data.species.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select just the rows desired from the 'describe' method and add in the 'median'\n",
    "stats_df = data.describe()\n",
    "\n",
    "stats_df.loc['range'] = stats_df.loc['max'] - stats_df.loc['min']\n",
    "\n",
    "out_fields = ['mean','25%','50%','75%', 'range']\n",
    "stats_df = stats_df.loc[out_fields]\n",
    "stats_df.rename({'50%': 'median'}, inplace=True)\n",
    "stats_df\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Calculate the following **for each species** in a separate dataframe:\n",
    "\n",
    "* The mean of each measurement (sepal_length, sepal_width, petal_length, and petal_width).\n",
    "* The median of each of these measurements.\n",
    "\n",
    "*Hint:* you may want to use Pandas [`groupby` method](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.groupby.html) to group by species before calculating the statistic.\n",
    "\n",
    "If you finish both of these, try calculating both statistics (mean and median) in a single table (i.e. with a single groupby call). See the section of the Pandas documentation on [applying multiple functions at once](http://pandas.pydata.org/pandas-docs/stable/groupby.html#applying-multiple-functions-at-once) for a hint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# The mean calculation\n",
    "data.groupby('species').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The median calculation\n",
    "data.groupby('species').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying multiple functions at once - 2 methods\n",
    "\n",
    "data.groupby('species').agg(['mean', 'median'])  # passing a list of recognized strings\n",
    "#data.groupby('species').agg([np.mean, np.median])  # passing a list of explicit aggregation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If certain fields need to be aggregated differently, we can do:\n",
    "from pprint import pprint\n",
    "\n",
    "agg_dict = {field: ['mean', 'median'] for field in data.columns if field != 'species'}\n",
    "agg_dict['petal_length'] = 'max'\n",
    "pprint(agg_dict)\n",
    "data.groupby('species').agg(agg_dict)\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5\n",
    "\n",
    "Make a scatter plot of `sepal_length` vs `sepal_width` using Matplotlib. Label the axes and give the plot a title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A simple scatter plot with Matplotlib\n",
    "ax = plt.axes()\n",
    "\n",
    "ax.scatter(data.sepal_length, data.sepal_width)\n",
    "\n",
    "# Label the axes\n",
    "ax.set(xlabel='Sepal Length (cm)',\n",
    "       ylabel='Sepal Width (cm)',\n",
    "       title='Sepal Length vs Width');\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6\n",
    "\n",
    "Make a histogram of any one of the four features. Label axes and title it as appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Using Matplotlib's plotting functionality\n",
    "ax = plt.axes()\n",
    "ax.hist(data.petal_length, bins=25);\n",
    "\n",
    "ax.set(xlabel='Petal Length (cm)', \n",
    "       ylabel='Frequency',\n",
    "       title='Distribution of Petal Lengths');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively using Pandas plotting functionality\n",
    "ax = data.petal_length.plot.hist(bins=25)\n",
    "\n",
    "ax.set(xlabel='Petal Length (cm)', \n",
    "       ylabel='Frequency',\n",
    "       title='Distribution of Petal Lengths');\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7\n",
    "\n",
    "Now create a single plot with histograms for each feature (`petal_width`, `petal_length`, `sepal_width`, `sepal_length`) overlayed. If you have time, next try to create four individual histogram plots in a single figure, where each plot contains one feature.\n",
    "\n",
    "For some hints on how to do this with Pandas plotting methods, check out the [visualization guide](http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html) for Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_context('notebook')\n",
    "### BEGIN SOLUTION\n",
    "# This uses the `.plot.hist` method\n",
    "ax = data.plot.hist(bins=25, alpha=0.5)\n",
    "ax.set_xlabel('Size (cm)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To create four separate plots, use Pandas `.hist` method\n",
    "axList = data.hist(bins=25, figsize=(20, 10))\n",
    "\n",
    "# Add some x- and y- labels to first column and last row\n",
    "for ax in axList.flatten():\n",
    "    if ax.get_subplotspec().is_last_row():\n",
    "        ax.set_xlabel('Size (cm)')\n",
    "        \n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel('Frequency')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8\n",
    "\n",
    "Using Pandas, make a boxplot of each petal and sepal measurement. Here is the documentation for [Pandas boxplot method](http://pandas.pydata.org/pandas-docs/version/0.18.1/visualization.html#visualization-box)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Here we have four separate plots\n",
    "data.boxplot(by='species', figsize=(12, 6));\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9\n",
    "\n",
    "Now make a single boxplot where the features are separated in the x-axis and species are colored with different hues. \n",
    "\n",
    "*Hint:* you may want to check the documentation for [Seaborn boxplots](http://seaborn.pydata.org/generated/seaborn.boxplot.html). \n",
    "\n",
    "Also note that Seaborn is very picky about data format--for this plot to work, the input dataframe will need to be manipulated so that each row contains a single data point (a species, a measurement type, and the measurement value). Check out Pandas [stack](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html) method as a starting place.\n",
    "\n",
    "Here is an example of a data format that will work:\n",
    "\n",
    "|   | species | measurement  | size |\n",
    "| - | ------- | ------------ | ---- |\n",
    "| 0\t| setosa  | sepal_length | 5.1  |\n",
    "| 1\t| setosa  | sepal_width  | 3.5  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# First we have to reshape the data so there is \n",
    "# only a single measurement in each column\n",
    "\n",
    "plot_data = (data\n",
    "             .set_index('species')\n",
    "             .stack()\n",
    "             .to_frame()\n",
    "             .reset_index()\n",
    "             .rename(columns={0:'size', 'level_1':'measurement'})\n",
    "            )\n",
    "\n",
    "plot_data.head()\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "# Now plot the dataframe from above using Seaborn\n",
    "\n",
    "sns.set_style('white')\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('dark')\n",
    "\n",
    "f = plt.figure(figsize=(10,8))\n",
    "sns.boxplot(x='measurement', y='size', \n",
    "            hue='species', data=plot_data);\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10\n",
    "\n",
    "Make a [pairplot](http://seaborn.pydata.org/generated/seaborn.pairplot.html) with Seaborn to examine the correlation between each of the measurements.\n",
    "\n",
    "*Hint:* this plot may look complicated, but it is actually only a single line of code. This is the power of Seaborn and dataframe-aware plotting! See the lecture notes for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### BEGIN SOLUTION\n",
    "sns.set_context('talk')\n",
    "sns.pairplot(data, hue='species')\n",
    "### END SOLUTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Exploratory Data Analysis**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **30** minutes\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the crucial process of using summary statistics and graphical representations to perform preliminary investigations on data to uncover patterns, detect anomalies, test hypotheses, and verify assumptions.\n",
    "\n",
    "In this notebook, we will learn some interesting and useful data exploration techniques that can be applied to explore any geographical data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*After completing this lab you will be able to:*\n",
    "\n",
    "*   Do Data Wrangling\n",
    "*   Do Data Filtering \n",
    "*   Plot with <code>plotly.express</code>\n",
    "*   Produce choropleth map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    " - [`pandas`](https://pandas.pydata.org/) for managing the data.\n",
    " - [`plotly.express`](https://plotly.com/python/plotly-express/) for visualizing the data.\n",
    " - [`json`](https://docs.python.org/3/library/json.html/) for reading json file formats.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Required Libraries**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required modules are pre-installed in the Skills Network Labs environment. However, if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import datetime \n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading and understanding our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset in this lab is <a href=\"https://www150.statcan.gc.ca/t1/tbl1/en/tv.action?pid=1810000101\">Monthly average retail prices for gasoline and fuel oil, by geography</a>  . It is available through Statistics Canada and includes monthly average gasoline price (Cents per Litre), of major Canadian Cities, starting from 1979 until recent. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another dataset, <a href=\"https://thomson.carto.com/tables/canada_provinces/public/map\">canada_provinces.geojson</a>, contains the mapping information of all Canadian Provinces. It will be used in our analysis to produce a choropleth map. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the data into *pandas* dataframe and look at the first 5 rows using the `head()` method. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline = pd.read_csv(\"data/gasoline_prices.csv\")\n",
    "gasoline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find out how many entries there are in our dataset, using `shape` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `info` function, we will take a look at our types of data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `columns` method, we will print all the column names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will check for any missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gasoline.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Wrangling** \n",
    "### Selecting and renaming the columns of interest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we are filtering our data, by selecting only the relevant columns. Also, we are using the `rename()` method to change the name of the columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (gasoline[['REF_DATE','GEO','Type of fuel','VALUE']]).rename(columns={\"REF_DATE\" : \"DATE\", \"Type of fuel\" : \"TYPE\"})\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `str.split()` function splits the string records, by a 'comma', with `n=1` slplit, and <code>Expend=True</code> , returns a dataframe. Below, we are splitting 'GEO' into 'City' and 'Province'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['City', 'Province']] = data['GEO'].str.split(',', n=1, expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing to *datetime* format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we scroll up to our `gasoline.info()` section, we can find that  'REF_DATE' is an object type. To be able to filter by day, month, or year, we need to change the format from object type to *datetime*. Pandas function `to_datetime()` transforms to date time format. Also, we need to specify the format of *datetime* that we need. In our case, `format='%b-%y'` means that it will split into the name of a month and year. `str.slice(stop=3)` splits and outputs the first 3 letters of a month. For more information on how to transform to *datetime*, please visit [this](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html) pandas documentation. Also, [this](https://strftime.org) web page contains more information on *datetime* formats.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['DATE'] = pd.to_datetime(data['DATE'], format='%b-%y')\n",
    "data['Month'] = data['DATE'].dt.month_name().str.slice(stop=3)\n",
    "data['Year'] = data['DATE'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function provides statistical information about the numeric variables. Since we only have the 'VALUE' variable that we want statistical information on, we will filter it by `data.VALUE.describe()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.VALUE.describe()\n",
    "# can also use  data['VALUE'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it is useful to know what is inside our categorical variables. We will use `unique().tolist()` functions to print out all of our 'GEO' colunm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.GEO.unique().tolist()\n",
    "# can also use  data['GEO'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, print out all categories in 'TYPE' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data.TYPE.unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "data.TYPE.unique().tolist()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Filtering** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will introduce you to some of the most common filtering techniques when working with pandas dataframes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering with logical operators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the logical operators on column values to filter rows. First, we  specify the name of our data, then, square brackets to select the name of the column, double 'equal' sign, '==' to select the name of a row group, in single or double quotation marks. If we want to exclude some entries (e.g. some locations), we would use the 'equal' and 'exclamation point' signs together, '=!'. We can also use '</>', '<=/>=' signs to select numeric information.\n",
    "\n",
    "Let's select the Calgary, Alberta data to see all the information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calgary = data[data['GEO'] == 'Calgary, Alberta']\n",
    "calgary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's select 2000 year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_years = data[data['Year'] ==  2000]\n",
    "sel_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by multiple conditions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many alternative ways to perform filtering in pandas. We can also use '|' ('or') and '&' (and) to select multiple columns and rows. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, let us select Toronto and Edmonton locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_loc = data[(data['GEO'] == \"Toronto, Ontario\") | (data['GEO'] == \"Edmonton, Alberta\")]\n",
    "mult_loc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use `isin` method to select multiple locations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = ['Calgary', 'Toronto', 'Edmonton']\n",
    "CTE = data[data.City.isin(cities)]\n",
    "CTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, please use the examples shown above, to select the data that shows the price of the 'household heating fuel', in Vancouver, in 1990.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below and run the cell\n",
    "price = data[(data['TYPE'] == 'Household heating fuel') & (data['City'] == 'Vancouver') & (data['Year'] == 1990)]\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "exercise2a = data[( data['Year'] ==  1990) & (data['TYPE'] == \"Household heating fuel\") & (data['City']=='Vancouver')]\n",
    "exercise2a\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, please select the data that shows the price of the 'household heating fuel', in Vancouver, in the years of 1979 and 2021.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below and run the cell\n",
    "# Enter your code below and run the cell\n",
    "price = data[(data['TYPE'] == 'Household heating fuel') & (data['City'] == 'Vancouver') & ((data['Year'] == 1979) | (data['Year'] == 2021))]\n",
    "price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "exercise2b = data[( data['Year'] <=  1979) | ( data['Year'] ==  2021) & (data['TYPE'] == \"Household heating fuel\") & (data['City']=='Vancouver')]\n",
    "exercise2b\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Hint</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "If we use '&' operator between the two years, it will return an empty data frame. This is because there was no data for the 'household heating fuel, in Vancouver, in 1979. Using 'or' operator is suitable because either one of two years that contains any information on 'household heating fuel' in Vancouver.\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering using `groupby()` method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of `groupby()` is to analyze data by some categories. The simplest call is by a column name. For example, letâ€™s use the 'GEO' column and `ngroups` function to calculate the number of groups (cities, provinces) in 'GEO' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = data.groupby('GEO')\n",
    "geo.ngroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most commonly, we use `groupby()` to split the data into groups,this will apply some function to each of the groups (e.g. mean, median, min, max, count), then combine the results into a data structure. For example, let's select the 'VALUE' column and calculate the mean of the gasoline prices per year. First, we specify the 'Year\" column, following by the 'VALUE' column, and the `mean()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_year = data.groupby(['Year'])['VALUE'].mean()\n",
    "group_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 a\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, please use `groupby()` method to group by the maximum value of gasoline prices, for each month. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below and run the cell\n",
    "ans = data.groupby(['Year', 'City'])['VALUE'].max()\n",
    "ans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "exercise3a = data.groupby(['Month'])['VALUE'].max()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, please use `groupby()` method to group by the median value of gasoline prices, for each year and each city. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below and run the cell\n",
    "price_bycity = data.groupby(['Year', 'City'])['VALUE'].median().reset_index(name ='Value').round(2)\n",
    "price_bycity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "exercise3b = data.groupby(['Year', 'City'])['VALUE'].median()\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Hint</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "    \n",
    "We can also reset the index of the new data output, by using `reset_index()`, and round up the output values to 2 decimal places.\n",
    "\n",
    "exercise3b = data.groupby(['Year', 'City'])['VALUE'].median().reset_index(name ='Value').round(2)\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing the data with *pandas* plotly.express** \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *plotly.express* library (usually imported as px) contains functions that can create entire figures at once. *plotly.express* is a built-in part of the *plotly* library, and makes creation of most common figures very easy. For more information on *plotly.express*, please refer to [this](https://plotly.com/python/plotly-express/) documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will plot the prices of gasoline in all cities during 1979 - 2021.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(price_bycity,\n",
    "                x='Year', y = \"Value\", \n",
    "                color = \"City\", color_discrete_sequence=px.colors.qualitative.Light24)\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(\n",
    "    title=\"Gasoline Price Trend per City\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Annual Average Price, Cents per Litre\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will plot the average monthly prices of gasoline in Toronto for the year of 2021.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mon_trend = data[(data['Year'] ==  2021) & (data['GEO'] == \"Toronto, Ontario\")]\n",
    "group_month = mon_trend.groupby(['Month'])['VALUE'].mean().reset_index().sort_values(by=\"VALUE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(group_month,\n",
    "                   x='Month', y = \"VALUE\")\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(\n",
    "    title=\"Toronto Average Monthly Gasoline Price in 2021\",\n",
    "    xaxis_title=\"Month\",\n",
    "    yaxis_title=\"Monthly Price, Cents per Litre\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, use *plotly.express* or other libraries, to plot the annual average gasoline price, per year, per gasoline type.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code below and run the cell\n",
    "type_gas = data.groupby(['Year', 'TYPE'])['VALUE'].mean().reset_index(name ='Type').round(2)\n",
    "fig = px.line(type_gas,\n",
    "                    x='Year', y = \"Type\", \n",
    "                    color = \"TYPE\", color_discrete_sequence=px.colors.qualitative.Light24)\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(\n",
    "    title=\"Fuel Type Price Trend\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Annual Average Price, Cents per Litre\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "type_gas = data.groupby(['Year', 'TYPE'])['VALUE'].mean().reset_index(name ='Type').round(2)\n",
    "fig = px.line(type_gas,\n",
    "                   x='Year', y = \"Type\", \n",
    "                   color = \"TYPE\", color_discrete_sequence=px.colors.qualitative.Light24)\n",
    "fig.update_traces(mode='markers+lines')\n",
    "fig.update_layout(\n",
    "    title=\"Fuel Type Price Trend\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Annual Average Price, Cents per Litre\")\n",
    "fig.show()\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the animated time frame to show the trend of gasoline prices over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bycity = data.groupby(['Year', 'City'])['VALUE'].mean().reset_index(name ='Value').round(2)\n",
    "bycity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.bar(bycity,  \n",
    "            x='City', y = \"Value\", animation_frame=\"Year\")\n",
    "fig.update_layout(\n",
    "    title=\"Time Lapse of Average Price of Gasoline, by Province\",\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Average Price of Gasoline, Cents per Litre\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to display the distribution of average gasoline prices in Canadian Provinces is by plotting a map. We will use 2021 year to display the average gasoline price in all Canadian Provinces.\n",
    "First, we select the year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_year = data[data['Year'] == 2021]\n",
    "one_year.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we group by the 'Province' and the 'mean' values of gasoline prices per each province. We also need to index each province with province id. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geodata =  one_year.groupby('Province')['VALUE'].mean().reset_index(name ='Average Gasoline Price').round(2)\n",
    "\n",
    "provinces={' Newfoundland and Labrador':5,\n",
    " ' Prince Edward Island':8,\n",
    " ' Nova Scotia':2,\n",
    " ' New Brunswick':7,\n",
    " ' Quebec':1,\n",
    " ' Ontario':11,\n",
    " ' Ontario part, Ontario/Quebec':12,\n",
    " ' Manitoba':10,\n",
    " ' Saskatchewan':3,\n",
    " ' Alberta':4,\n",
    " ' British Columbia':6,\n",
    " ' Yukon':9,\n",
    " ' Northwest Territories':13\n",
    "}\n",
    "geodata['ProvinceID']=geodata['Province'].map(provinces)\n",
    "display(geodata)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are linking each province by its specified 'provinceID' with another dataset, â€˜canada_provinces.geojsonâ€™, containing all the mapping information for plotting our provinces.\n",
    "\n",
    "First, we need to download the Canadian Provinces dataset from IBM cloud storage, using the `requests.get()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#geo = requests.get(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBM-ML0232EN-SkillsNetwork/asset/canada_provinces.geojson\").text\n",
    "geo = open(\"data/canada_provinces.geojson\", \"r\").read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load the file as a string, using `json.loads()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp = json.loads(geo)\n",
    "    \n",
    "fig = px.choropleth(geodata,\n",
    "                    locations=\"ProvinceID\",\n",
    "                    geojson=mp,\n",
    "                    featureidkey=\"properties.cartodb_id\",\n",
    "                    color=\"Average Gasoline Price\",\n",
    "                    color_continuous_scale=px.colors.diverging.Tropic,\n",
    "                    scope='north america',\n",
    "                    title='<b>Average Gasoline Price </b>',                \n",
    "                    hover_name='Province',\n",
    "                    hover_data={\n",
    "                        'Average Gasoline Price' : True,\n",
    "                        'ProvinceID' : False\n",
    "                    },\n",
    "                    locationmode='geojson-id',\n",
    "                    )\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    legend_title_text='<b>Average Gasoline Price</b>',\n",
    "    font={\"size\": 16, \"color\": \"#808080\", \"family\" : \"calibri\"},\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "    legend=dict(orientation='v'),\n",
    "    geo=dict(bgcolor='rgba(0,0,0,0)', lakecolor='#e0fffe')\n",
    ")\n",
    "\n",
    "#Show Canada only \n",
    "fig.update_geos(showcountries=False, showcoastlines=False,\n",
    "                showland=False, fitbounds=\"locations\",\n",
    "                subunitcolor='white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, experiment with different color scales to make the visualization easier to read. Some suggestions are provided in the \"Hint\" section. Simply copy the above code and replace 'px.colors.diverging.Tropic', with any other color scales. For example, the sequential color scales are appropriate for most continuous data, but in some cases it can be helpful to use a diverging or cyclical color scale. Diverging color scales are appropriate for the continuous data that has a natural midpoint. For more information on *plotly* colors, please visit [this plotly documentation](https://plotly.com/python/builtin-colorscales/) web page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "mp = json.loads(geo)\n",
    "    \n",
    "fig = px.choropleth(geodata,\n",
    "                    locations=\"ProvinceID\",\n",
    "                    geojson=mp,\n",
    "                    featureidkey=\"properties.cartodb_id\",\n",
    "                    color=\"Average Gasoline Price\",\n",
    "                    color_continuous_scale=px.colors.diverging.Temps,\n",
    "                    scope='north america',\n",
    "                    title='<b>Average Gasoline Price </b>',                \n",
    "                    hover_name='Province',\n",
    "                    hover_data={\n",
    "                        'Average Gasoline Price' : True,\n",
    "                        'ProvinceID' : False\n",
    "                    },\n",
    "                    locationmode='geojson-id',\n",
    "                    )\n",
    "fig.update_layout(\n",
    "    showlegend=True,\n",
    "    legend_title_text='<b>Average Gasoline Price</b>',\n",
    "    font={\"size\": 16, \"color\": \"#808080\", \"family\" : \"calibri\"},\n",
    "    margin={\"r\":0,\"t\":40,\"l\":0,\"b\":0},\n",
    "    legend=dict(orientation='v'),\n",
    "    geo=dict(bgcolor='rgba(0,0,0,0)', lakecolor='#e0fffe')\n",
    ")\n",
    "\n",
    "#Show Canada only \n",
    "fig.update_geos(showcountries=False, showcoastlines=False,\n",
    "                showland=False, fitbounds=\"locations\",\n",
    "                subunitcolor='white')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Hint</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "    px.colors.diverging.Tropic\n",
    "    px.colors.diverging.Temps\n",
    "    px.colors.sequential.Greens\n",
    "    px.colors.sequential.Reds\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1, Part d: Feature Engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering with Linear Regression: Applied to the Ames Housing Data\n",
    "\n",
    "Using the Ames Housing Data:\n",
    "\n",
    "Dean De Cock\n",
    "Truman State University\n",
    "Journal of Statistics Education Volume 19, Number 3(2011), www.amstat.org/publications/jse/v19n3/decock.pdf\n",
    "\n",
    "In this notebook, we will build some linear regression models to predict housing prices from this data. In particular, we will set out to improve on a baseline set of features via **feature engineering**: deriving new features from our existing data. Feature engineering often makes the difference between a weak model and a strong one.\n",
    "\n",
    "We will use visual exploration, domain understanding, and intuition to construct new features that will be useful later in the course as we turn to prediction.\n",
    "\n",
    "**Notebook Contents**\n",
    "\n",
    "> 1. Simple EDA \n",
    "> 2. One-hot Encoding variables\n",
    "> 3. Log transformation for skewed variables\n",
    "> 4. Pair plot for features\n",
    "> 5. Basic feature engineering: adding polynomial and interaction terms\n",
    "> 6. Feature engineering: categories and features derived from category aggregates \n",
    "\n",
    "## 1. Simple EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['retina']\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the Data, Examine and Explore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load in the Ames Housing Data\n",
    "datafile = \"data/Ames_Housing_Data.tsv\"\n",
    "df = pd.read_csv(datafile, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Examine the columns, look at missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gr Liv Area'].hist()\n",
    "df[df['Gr Liv Area']>4000]['Gr Liv Area']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is recommended by the data set author to remove a few outliers\n",
    "\n",
    "df = df.loc[df['Gr Liv Area'] <= 4000,:]\n",
    "print(\"Number of rows in the data:\", df.shape[0])\n",
    "print(\"Number of columns in the data:\", df.shape[1])\n",
    "data = df.copy() # Keep a copy our original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A quick look at the data:\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(df))\n",
    "print(len(df.Order.unique()))\n",
    "print(len(df.PID.unique()))\n",
    "\n",
    "# As columns Order and PID has unique values for each row, we can drop them\n",
    "# axis=1 means we are dropping columns instead of rows (default)\n",
    "# inplace=True means we are modifying the dataframe directly\n",
    "df.drop(['Order', 'PID'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to first do some basic data cleaning on this data: \n",
    "\n",
    "* Making skew variables symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log transforming skew variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of float colums to check for skewing\n",
    "num_cols = df.select_dtypes('number').columns\n",
    "\n",
    "skew_limit = 0.75 # define a limit above which we will log transform\n",
    "skew_vals = df[num_cols].skew()\n",
    "skew_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing the skewed columns\n",
    "skew_cols = skew_vals[abs(skew_vals) > skew_limit].sort_values(ascending=False)\n",
    "skew_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at what happens to one of these features, when we apply np.log1p visually.\n",
    "\n",
    "# Choose a field\n",
    "field = \"SalePrice\"\n",
    "\n",
    "# Create two \"subplots\" and a \"figure\" using matplotlib\n",
    "fig, (ax_first, ax_second) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Create a histogram on the \"ax_first\" subplot\n",
    "df[field].hist(ax=ax_first)\n",
    "\n",
    "# Apply a log transformation (numpy syntax) to this column\n",
    "# Create a histogram on the \"ax_seconf\" subplot\n",
    "# np.log1p vs np.log -- np.log1p is preferred as it do log(1+x) instead of log(x)\n",
    "# this is useful when x is 0\n",
    "df[field].apply(np.log1p).hist(ax=ax_second)\n",
    "\n",
    "# Formatting of titles etc. for each subplot\n",
    "ax_first.set(title='Before np.log1p', ylabel='frequency', xlabel='value')\n",
    "ax_second.set(title='After np.log1p', ylabel='frequency', xlabel='value')\n",
    "fig.suptitle(f'Field \"{field}\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the skew transformation:\n",
    "\n",
    "for col in skew_cols.index.values:\n",
    "    if col == \"SalePrice\":\n",
    "        continue\n",
    "    df[col] = df[col].apply(np.log1p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now have a larger set of potentially-useful features\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are a *lot* of variables. Let's go back to our saved original data and look at how many values are missing for each variable. \n",
    "df = data\n",
    "data.isnull().sum().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Let's pick out just a few numeric columns to illustrate basic feature transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df= df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                      'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                      'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                      'Garage Cars','SalePrice']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can look at summary statistics of the subset data\n",
    "smaller_df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There appears to be one NA in Garage Cars - we will take a simple approach and fill it with 0\n",
    "smaller_df = smaller_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smaller_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pair plot of features\n",
    "Now that we have a nice, filtered dataset, let's generate visuals to better understand the target and feature-target relationships: pairplot is great for this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(smaller_df, plot_kws=dict(alpha=.1, edgecolor='none'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Data Exploration Discussion**: \n",
    "\n",
    "1. What do these plots tell us about the distribution of the target?   \n",
    "\n",
    "2. What do these plots tell us about the relationship between the features and the target? Do you think that linear regression is well-suited to this problem? Do any feature transformations come to mind?\n",
    "\n",
    "3. What do these plots tell us about the relationship between various pairs of features? Do you think there may be any problems here? \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Suppose our target variable is the SalePrice. We can set up separate variables for features and target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separate our features from our target\n",
    "\n",
    "X = smaller_df.loc[:,['Lot Area', 'Overall Qual', 'Overall Cond', \n",
    "                        'Year Built', 'Year Remod/Add', 'Gr Liv Area', \n",
    "                        'Full Bath', 'Bedroom AbvGr', 'Fireplaces', \n",
    "                        'Garage Cars']]\n",
    "\n",
    "y = smaller_df['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have feature/target data X, y ready to go, we're nearly ready to fit and evaluate a baseline model using our current feature set. We'll need to create a **train/validation split** before we fit and score the model. \n",
    "\n",
    "Since we'll be repeatedly splitting X, y into the same train/val partitions and fitting/scoring new models as we update our feature set, we'll define a reusable function that completes all these steps, making our code/process more efficient going forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's go ahead and run this function on our baseline feature set and take some time to analyze the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic feature engineering: adding polynomial and interaction terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the first things that we looked for in the pairplot was evidence about the relationship between each feature and the target. In certain features like _'Overall Qual'_ and _'Gr Liv Qual'_, we notice an upward-curved relationship rather than a simple linear correspondence. This suggests that we should add quadratic **polynomial terms or transformations** for those features, allowing us to express that non-linear relationship while still using linear regression as our model.\n",
    "\n",
    "Luckily, pandas makes it quite easy to quickly add those square terms as additional features to our original feature set. We'll do so and evaluate our model again below.\n",
    "\n",
    "As we add to our baseline set of features, we'll create a copy of the latest benchmark so that we can continue to store our older feature sets. \n",
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = X.copy()\n",
    "\n",
    "X2['OQ2'] = X2['Overall Qual'] ** 2\n",
    "X2['GLA2'] = X2['Gr Liv Area'] ** 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is, each feature is treated as an independent quantity. However, there may be **interaction effects**, in which the impact of one feature may dependent on the current value of a different feature.\n",
    "\n",
    "For example, there may be a higher premium for increasing _'Overall Qual'_ for houses that were built more recently. If such a premium or a similar effect exists, a feature that multiplies _'Overall Qual'_ by _'Year Built'_ can help us capture it.\n",
    "\n",
    "Another style of interaction term involves feature proprtions: for example, to get at something like quality per square foot we could divide _'Overall Qual'_ by _'Lot Area'_.\n",
    "\n",
    "Let's try adding both of these interaction terms and see how they impact the model results.\n",
    "\n",
    "### Feature interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X3 = X2.copy()\n",
    "\n",
    "# multiplicative interaction\n",
    "X3['OQ_x_YB'] = X3['Overall Qual'] * X3['Year Built']\n",
    "\n",
    "# division interaction\n",
    "X3['OQ_/_LA'] = X3['Overall Qual'] / X3['Lot Area']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----\n",
    "**Interaction Feature Exercise**: What other interactions do you think might be helpful? Why? \n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categories and features derived from category aggregates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Incorporating **categorical features** into linear regression models is fairly straightforward: we can create a new feature column for each category value, and fill these columns with 1s and 0s to indicate which category is present for each row. This method is called **dummy variables** or **one-hot-encoding**.\n",
    "\n",
    "We'll first explore this using the _'House Style'_ feature from the original dataframe. Before going straight to dummy variables, it's a good idea to check category counts to make sure all categories have reasonable representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a Pd.Series consisting of all the string categorical features\n",
    "df2 = data.copy()\n",
    "one_hot_encode_cols = df2.select_dtypes(include=['object'])  # filtering by string categorical features\n",
    "\n",
    "one_hot_encode_cols.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the one hot encoding\n",
    "print(df2.head())\n",
    "df2 = pd.get_dummies(df2, columns=one_hot_encode_cols.columns.to_list(), drop_first=True)\n",
    "df2.columns.to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One shot encoding of single column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['House Style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks ok, and here's a quick look at how dummy features actually appear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.get_dummies(df['House Style'], drop_first=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call `pd.get_dummies()` on our entire dataset to quickly get data with all the original features and dummy variable representation of any categorical features. Let's look at some variable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbh_counts = df.Neighborhood.value_counts()\n",
    "nbh_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this category, let's map the few least-represented neighborhoods to an \"other\" category before adding the feature to our feature set and running a new benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_nbhs = list(nbh_counts[nbh_counts <= 8].index)\n",
    "\n",
    "other_nbhs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X4 = X3.copy()\n",
    "\n",
    "X4['Neighborhood'] = df['Neighborhood'].replace(other_nbhs, 'Other')\n",
    "\n",
    "X4.Neighborhood.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting to fancier features\n",
    "\n",
    "Let's close out our introduction to feature engineering by considering a more complex type of feature that may work very nicely for certain problems. It doesn't seem to add a great deal over what we have so far, but it's a style of engineering to keep in mind for the future.\n",
    "\n",
    "We'll create features that capture where a feature value lies relative to the members of a category it belongs to. In particular, we'll calculate deviance of a row's feature value from the mean value of the category that row belongs to. This helps to capture information about a feature relative to the category's distribution, e.g. how nice a house is relative to other houses in its neighborhood or of its style.\n",
    "\n",
    "Below we define reusable code for generating features of this form, feel free to repurpose it for future feature engineering work!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_deviation_feature(X, feature, category):\n",
    "    \n",
    "    # temp groupby object\n",
    "    category_gb = X.groupby(category)[feature]\n",
    "    \n",
    "    # create category means and standard deviations for each observation\n",
    "    category_mean = category_gb.transform(lambda x: x.mean())\n",
    "    category_std = category_gb.transform(lambda x: x.std())\n",
    "    \n",
    "    # compute stds from category mean for each feature value,\n",
    "    # add to X as new feature\n",
    "    deviation_feature = (X[feature] - category_mean) / category_std \n",
    "    X[feature + '_Dev_' + category] = deviation_feature  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's use our feature generation code to add 2 new deviation features, and run a final benchmark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X5 = X4.copy()\n",
    "X5['House Style'] = df['House Style']\n",
    "add_deviation_feature(X5, 'Year Built', 'House Style')\n",
    "add_deviation_feature(X5, 'Overall Qual', 'Neighborhood')\n",
    "\n",
    "X5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Features in Scikit-Learn\n",
    "\n",
    "`sklearn` allows you to build many higher-order terms at once with `PolynomialFeatures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate and provide desired degree; \n",
    "#   Note: degree=2 also includes intercept, degree 1 terms, and cross-terms\n",
    "\n",
    "pf = PolynomialFeatures(degree=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Lot Area', 'Overall Qual']\n",
    "pf.fit(df[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_array = pf.transform(df[features])\n",
    "pd.DataFrame(feat_array, columns = pf.get_feature_names_out(input_features=features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "While we haven't yet turned to prediction, these feature engineering exercises set the stage. Generally, feature engineering often follows a sort of [_Pareto principle_](https://en.wikipedia.org/wiki/Pareto_principle), where a large bulk of the predictive gains can be reached through adding a set of intuitive, strong features like polynomial transforms and interactions. Directly incorporating additional information like categorical variables can also be very helpful. Beyond this point, additional feature engineering can provide significant, but potentially diminishing returns. Whether it's worth it depends on the use case for the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Feature Engineering**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimated time needed: **45** minutes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A critical part of the successful Machine Learning project is coming up with a good set of features to train on. This process is called feature engineering, and it involves three steps: feature transformation (transforming the original features), feature selection (selecting the most useful features to train on), and feature extraction (combining existing features to produce more useful ones). In this notebook we will explore different tools in Feature Engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing this lab you will be able to:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*   Understand the types of Feature Engineering\n",
    "    *   Feature Transformation\n",
    "        *   Dealing with Categorical Variables\n",
    "            *   One Hot Encoding\n",
    "            *   Label Encoding\n",
    "        *   Date Time Transformations\n",
    "    *   Feature Selection\n",
    "    *   Feature Extraction using Principal Component Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Setup**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the following libraries:\n",
    "\n",
    "*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for managing the data.\n",
    "*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for mathematical operations.\n",
    "*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for visualizing the data.\n",
    "*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for visualizing the data.\n",
    "*   [`plotly.express`](https://plotly.com/python/plotly-express/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for visualizing the data.\n",
    "*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01) for machine learning and machine-learning-pipeline related functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Required Libraries**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following required modules are pre-installed in the Skills Network Labs environment. However if you run this notebook commands in a different Jupyter environment (e.g. Watson Studio or Ananconda) you will need to install these libraries by removing the `#` sign before `!mamba` in the code cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n",
    "# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n",
    "# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python library to work with excel files\n",
    "# pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Surpress warnings from using older version of sklearn:\n",
    "# def warn(*args, **kwargs):\n",
    "#     pass\n",
    "# import warnings\n",
    "# warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Reading and understanding our data**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we will be using the airlines_data.xlsx file, hosted on IBM Cloud object. This dataset contains the prices of flight tickets for various airlines between the months of March and June of 2019 and between various cities. This dataset is often used for prediction analysis of the flight prices which are influenced by various factors, such as name of the airline, date of journey, route, departure and arrival times, the source and the destination of the trip, duration and other parameters.\n",
    "\n",
    "In this notebook, we will use the airlines dataset to perform feature engineering on some of its independent variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by reading the data into *pandas* data frame and looking at the first 5 rows using the `head()` method.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel('data/airlines_data.xlsx')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using the `info` function, we will take a look at the types of data that our dataset contains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we see from the output above, we mostly have object data types, except for the 'price' column, which is an integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `describe()` function provides the statistical information about the numerical variables. In our case, it is the 'price' variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will check for any null values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have found some null points, we need to either remove them from our dataset or fill them with something else. In this case, we will use `fillna()` and `method='ffill'`, which fills the last observed non-null value forward until another non-null value is encountered.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.ffill()\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Transformation**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Transformation means transforming our features to the functions of the original features. For example, feature encoding, scaling, and discretization (the process of transforming continuous variables into discrete form, by creating bins or intervals) are the most common forms of data transformation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dealing with Categorical Variables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical variables represent qualitative data with no apparent inherent mathematical meaning. Therefore, for any machine learning analysis, all the categorical data must be transformed into the numerical data types. First, we'll start with 'Airlines' column, as it contains categorical values. We will use `unique()` method to obtain all the categories in this column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airlines = data['Airline'].unique().tolist()\n",
    "airlines.sort()\n",
    "airlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above list, we notice that some of the airline names are being repeated. For example, 'Jet Airways' and 'Jet Airways Business'. This means that some of the airlines are subdivided into separate parts. We will combine these 'two-parts' airlines to make our categorical features more consistent with the rest of the variables.\n",
    "\n",
    "Here, we will use the *numpy* `where()` function to locate and combine the two categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Airline'] = np.where(data['Airline']=='Vistara Premium economy', 'Vistara', data['Airline'])\n",
    "data['Airline'] = np.where(data['Airline']=='Jet Airways Business', 'Jet Airways', data['Airline'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "In this exercise, use `np.where()` function to combine 'Multiple carriers Premium economy' and 'Multiple carriers' categories, like shown in the code above. Print the newly created list using `unique().tolist()` functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data['Airline'] = np.where(data['Airline']=='Multiple carriers Premium economy', 'Multiple carriers', data['Airline'])\n",
    "\n",
    "data['Airline'].unique().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "data\\['Airline'] = np.where(data\\['Airline']=='Multiple carriers Premium economy', 'Multiple carriers', data\\['Airline'])\n",
    "data\\['Airline'].unique().tolist()\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **One Hot Encoding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to be recognized by a machine learning algorithms, our categorical variables should be converted into numerical ones. One way to do this is through *one hot encoding*. To learn more about this process, please visit this [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01).\n",
    "\n",
    "We will use, `get_dummies()` method to do this transformation. In the next cell, we will transform 'Airline', 'Source', and 'Destination' into their respective numeric variables. We will put all the transformed data into a 'data1' data frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.get_dummies(data=data, columns = ['Airline', 'Source', 'Destination'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we will compare our original data frame with the transformed one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, we went from 11 original features in our dataset to 28. This is because *Pandas* `get_dummies()` approach when applied to a column with different categories (e.g. different airlines) will produce a new column (variable) for each unique categorical value (for each unique airline). It will place a one in the column corresponding to the categorical value present for that observation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2\n",
    "\n",
    "In this exercise, use `value_counts()` to determine the values distribution of the 'Total_Stops' parameter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data1.Total_Stops.value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "data\\[\"Total_Stops\"].value_counts()\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Label Encoding**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since 'Total_Stops' is originally a categorical data type, we also need to convert it into numerical one. For this, we can perform a label encoding, where values are manually assigned to the corresponding keys, like \"0\" to a \"non-stop\", using the `replace()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.replace({\"non-stop\":0,\"1 stop\":1,\"2 stops\":2,\"3 stops\":3,\"4 stops\":4},inplace=True)\n",
    "data1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Date Time Transformations**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transforming the 'Duration' time column**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will take a closer look at the 'Duration' variable. Duration is the time taken by a plane to reach its destination. It is the difference between the 'Dep_Time' and 'Arrival_Time'. In our dataset, the 'Duration' is expressed as a string, in hours and minutes. To be recognized by machine learning algorithms, we also need to transform it into numerical type.\n",
    "\n",
    "The code below will iterate through each record in 'Duration' column and split it into hours and minutes, as two additional separate columns. Also, we want to add the 'Duration_hours' (in minutes) to the 'Duration_minutes' column to obtain a 'Duration_Total_mins' time, in minutes. The total duration time column will be useful feature for any regression type of analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = list(data1['Duration'])\n",
    "\n",
    "# Checking if if the duration does not have 'h' or 'm' then add '0h' or '0m' respectively\n",
    "for i in range(len(duration)) :\n",
    "    if len(duration[i].split()) != 2:\n",
    "        if 'h' in duration[i] :\n",
    "            duration[i] = duration[i].strip() + ' 0m'\n",
    "        elif 'm' in duration[i] :\n",
    "            duration[i] = '0h {}'.format(duration[i].strip())\n",
    "            \n",
    "dur_hours = []\n",
    "dur_minutes = []  \n",
    "# Extracting hours and minutes from duration\n",
    "for i in range(len(duration)) :\n",
    "    dur_hours.append(int(duration[i].split()[0][:-1]))\n",
    "    dur_minutes.append(int(duration[i].split()[1][:-1]))\n",
    "\n",
    "\n",
    "data1['Duration_hours'] = dur_hours\n",
    "data1['Duration_minutes'] =dur_minutes\n",
    "data1.loc[:,'Duration_hours'] *= 60\n",
    "data1['Duration_Total_mins']= data1['Duration_hours']+data1['Duration_minutes']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print 'data1' data frame to see the newly created columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you have noticed, three new columns were created: 'Duration_hours', 'Duration_minutes', and 'Duration_Total_mins' - all numerical values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transforming the 'Departure' and 'Arrival' Time Columns**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will transform the 'Dep_Time' and 'Arrival_Time' columns to the appropriate date and time format. We will use *pandas* `to_datetime()` function for this.\n",
    "\n",
    "We will split the 'Dep_Time' and 'Arrival_Time' columns into their corresponding hours and minutes columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1[\"Dep_Hour\"]= pd.to_datetime(data1['Dep_Time'], format=\"%H:%M\").dt.hour\n",
    "data1[\"Dep_Min\"]= pd.to_datetime(data1['Dep_Time'], format=\"%H:%M\").dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3\n",
    "\n",
    "Now, let's transform the 'Arrival_Time' column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data1[\"Arrival_Hour\"]= pd.to_datetime(data1['Arrival_Time'], format=\"mixed\").dt.hour\n",
    "data1[\"Arrival_Min\"]= pd.to_datetime(data1['Arrival_Time'], format=\"mixed\").dt.minute"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "data1\\[\"Arrival_Hour\"]= pd.to_datetime(data1\\['Arrival_Time']).dt.hour\n",
    "data1\\[\"Arrival_Min\"]= pd.to_datetime(data1\\['Arrival_Time']).dt.minute\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Splitting 'Departure/Arrival_Time' into Time Zones**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further transform our 'Departure/Arrival_Time' column, we can break down the 24 hours format for the departure and arrival time into 4 different time zones: night, morning, afternoon, and evening. This might be an interesting feature engineering technique to see what time of a day has the most arrivals/departures.\n",
    "\n",
    "One way to do this is transformation is by using *pandas* `cut()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['dep_timezone'] = pd.cut(data1.Dep_Hour, [0,6,12,18,24], labels=['Night','Morning','Afternoon','Evening'])\n",
    "data1['dep_timezone'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4\n",
    "\n",
    "Now, let's transform the 'Arrival_Time' column into its corresponding time zones, as shown in the example above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data1['arrival_timezone'] = pd.cut(data1.Arrival_Hour, [0,6,12,18,24], labels=['Night','Morning','Afternoon','Evening'])\n",
    "data1['arrival_timezone']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "data1\\[\"Arrival_Hour\"]= pd.to_datetime(data1\\['Arrival_Time']).dt.hour\n",
    "data1\\['arr_timezone'] = pd.cut(data1.Arrival_Hour, \\[0,6,12,18,24], labels=\\['Night','Morning','Afternoon','Evening'])\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Transforming the 'Date_of_Journey' Column**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the departure/arrival time, we will now extract some information from the 'date_of_journey' column, which is also an object type and can not be used for any machine learning algorithm yet.\n",
    "\n",
    "So, we will extract the month information first and store it under the 'Month' column name.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['Month']= pd.to_datetime(data1[\"Date_of_Journey\"], format=\"%d/%m/%Y\").dt.month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5\n",
    "\n",
    "Now, let's create 'Day' and 'Year' columns in a similar way.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "data1['Year']= pd.to_datetime(data1[\"Date_of_Journey\"], format=\"%d/%m/%Y\").dt.year\n",
    "data1['Day']= pd.to_datetime(data1[\"Date_of_Journey\"], format=\"%d/%m/%Y\").dt.day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "data1\\['Day']= pd.to_datetime(data1\\[\"Date_of_Journey\"], format=\"%d/%m/%Y\").dt.day\n",
    "data1\\['Year']= pd.to_datetime(data1\\[\"Date_of_Journey\"], format=\"%d/%m/%Y\").dt.year\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can extract the day of the weak name by using `dt.day_name()` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1['day_of_week'] = pd.to_datetime(data1['Date_of_Journey'], format=\"%d/%m/%Y\").dt.day_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Selection**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will select only those attributes which best explain the relationship of the independent variables with respect to the target variable, 'price'. There are many methods for feature selection, building the heatmap and calculating the correlation coefficients scores are the most commonly used ones.\n",
    "\n",
    "First, we will select only the relevant and newly transformed variables (and exclude variables such as 'Route', 'Additional_Info', and all the original categorical variables), and place them into a 'new_data' data frame.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will print all of our data1 columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = data1.loc[:,['Total_Stops', 'Airline_Air Asia',\n",
    "       'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
    "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_SpiceJet',\n",
    "       'Airline_Trujet', 'Airline_Vistara', 'Source_Banglore',\n",
    "       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n",
    "       'Destination_Banglore', 'Destination_Cochin', 'Destination_Delhi',\n",
    "       'Destination_Hyderabad', 'Destination_Kolkata', 'Destination_New Delhi',\n",
    "       'Duration_hours', 'Duration_minutes', 'Duration_Total_mins', 'Dep_Hour',\n",
    "       'Dep_Min', 'Price']]\n",
    "\n",
    "new_data.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will construct a `heatmap()`, using the *seaborn* library with a newly formed data frame, 'new_data'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,18))\n",
    "sns.heatmap(new_data.corr(),annot=True,cmap='RdYlGn')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the heatmap above, extreme green means highly positively correlated features (relationship between two variables in which both variables move in the same direction), extreme red means negatively correlated features (relationship between two variables in which an increase in one variable is associated with a decrease in the other).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can use the `corr()` function to calculate and list the correlation between all independent variables and the 'price'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = new_data.corr()['Price'].sort_values()\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot these correlation coefficients for easier visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.plot(kind='bar',figsize=(15,8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the graph above, we can deduct some of the highly correlated features and select only those ones for any future analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Feature Extraction using Principal Component Analysis (Optional)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PCA with Scikit-Learn**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dimentionality reduction is part of the feature extraction process that combines the existing features to produce more useful ones. The goal of dimensionality reduction is to simplify the data without loosing too much information. Principal Component Analysis (PCA) is one of the most popular dimensionality reduction algorithms. First, it identifies the hyperplane that lies closest to the data, and then it projects the data onto it. In this way, a few multidimensional features are merged into one.\n",
    "\n",
    "In the following portion of the lab, we will use `scikit-learn` library to perform some PCA on our data.\n",
    "To learn more about `scikit-learn` PCA, please visit this [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0232ENSkillsNetwork30654641-2022-01-01#sklearn.decomposition.PCA).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we must scale our data using the `StandardScaler()` function.\n",
    "We will assign all the independent variables to x, and the dependent variable, 'price', to y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data1.loc[:,['Total_Stops', 'Airline_Air Asia',\n",
    "       'Airline_Air India', 'Airline_GoAir', 'Airline_IndiGo',\n",
    "       'Airline_Jet Airways', 'Airline_Multiple carriers', 'Airline_SpiceJet',\n",
    "       'Airline_Trujet', 'Airline_Vistara', 'Source_Banglore',\n",
    "       'Source_Chennai', 'Source_Delhi', 'Source_Kolkata', 'Source_Mumbai',\n",
    "       'Destination_Banglore', 'Destination_Cochin', 'Destination_Delhi',\n",
    "       'Destination_Hyderabad', 'Destination_Kolkata', 'Destination_New Delhi',\n",
    "       'Duration_hours', 'Duration_minutes', 'Duration_Total_mins', 'Dep_Hour',\n",
    "       'Dep_Min']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y= data1.Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x=scaler.fit_transform(x.astype(np.float64))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is scaled, we can apply the `fit_transform()` function to reduce the dimensionality of the dataset down to two dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 2)\n",
    "pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Explained Variance Ratio**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful piece of information in PCA is the explained variance ratio of each principal component, available via the `explained_variance_ratio_` function. The ratio indicates the proportion of the dataset's variance that lies along each principal component. Let's look at the explained variance ratio of each of our two components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance=pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first component constitutes 17.54% of the variance and second component constitutes 12.11% of the variance between the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6 (Optional)\n",
    "\n",
    "In this exercise, experiment with the number of components to see how many dimensions our dataset could be reduced to in order to explain most of the variability between the features. Additionally, you can plot the components using bar plot to see how much variability each component represents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "pca = PCA(n_components = 7)\n",
    "pca.fit_transform(x)\n",
    "explained_variance=pca.explained_variance_ratio_\n",
    "explained_variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your code and run the cell\n",
    "with plt.style.context('dark_background'):\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    plt.bar(range(7), explained_variance, alpha=0.5, align='center', \n",
    "    label='individual explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution_part1</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "pca = PCA(n_components = 7)\n",
    "pca.fit_transform(x)\n",
    "explained_variance=pca.explained_variance_ratio\\_\n",
    "explained_variance\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><strong>Solution_part2</strong> (Click Here)</summary>\n",
    "    &emsp; &emsp; <code>\n",
    "\n",
    "with plt.style.context('dark_background'):\n",
    "    \n",
    "    plt.figure(figsize=(6, 4))\n",
    "    \n",
    "    plt.bar(range(7), explained_variance, alpha=0.5, align='center', \n",
    "    label='individual explained variance')\n",
    "    plt.ylabel('Explained variance ratio')\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout() \n",
    "\n",
    "\n",
    "</code>\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Choosing the Right Number of Dimensions**\n",
    "\n",
    "Instead of arbitrary choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large proportion of the variance, let's say 95%.\n",
    "\n",
    "The following code performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the variance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(x)\n",
    "cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "d = np.argmax(cumsum >=0.95) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 16 components required to meet 95% variance. Therefore, we could set n_components = 16 and run PCA again. However, there is better way, instead of specifying the number of principal components you want to preserve, you can set n_components to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=0.95)\n",
    "x_reduced = pca.fit_transform(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a graphical way to determine the number of principal components in your analysis. It is to plot the explained variance as a function of the number of dimensions. There will usually be an elbow in the curve, where the explained variance stops growing fast. That point is usually the optimal point for the number of principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.area(\n",
    "    x=range(1, cumsum.shape[0] + 1),\n",
    "    y=cumsum,\n",
    "    labels={\"x\": \"# Components\", \"y\": \"Explained Variance\"}\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
